{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"D:/Final ADM/Extracted dataset/\")\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "import cars_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from multiprocessing import Pool\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHOOSING 10 CLASSES OF CARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = {\n",
    "    \"honda_civic_1998\": [\"honda_civic_1997\", \"honda_civic_1998\"], # available \"honda_civic_1999\"\n",
    "    \"honda_accord_1997\": [\"honda_accord_1996\", \"honda_accord_1997\"],\n",
    "    \"ford_f150_2006\": [\"ford_f150_2005\", \"ford_f150_2006\", \"ford_f150_2007\"],\n",
    "    \"chevrolet_silverado_2004\": [\"chevrolet_silverado_2003\", \"chevrolet_silverado_2004\"], # available \"chevrolet_silverado_2005\"\n",
    "    \"toyota_camry_2014\": [\"toyota_camry_2012\", \"toyota_camry_2013\", \"toyota_camry_2014\", \"toyota_camry_le_2012\", \"toyota_camry_le_2013\", \"toyota_camry_le_2014\", \"toyota_camry_se_2012\", \"toyota_camry_se_2013\", \"toyota_camry_xle_2012\", \"toyota_camry_xle_2013\"],\n",
    "    \"nissan_altima_2014\": [\"nissan_altima_2013\", \"nissan_altima_2014\", \"nissan_altima_2015\"],\n",
    "    \"toyota_corolla_2013\": [\"toyota_corolla_2011\", \"toyota_corolla_2012\", \"toyota_corolla_2013\", \"toyota_corolla_ce_2012\", \"toyota_corolla_le_2012\", \"toyota_corolla_le_2013\", \"toyota_corolla_s_2011\", \"toyota_corolla_s_2012\"],\n",
    "    \"dodge_ram_2001\": [\"dodge_ram_1500_2000\", \"dodge_ram_1500_2001\", \"dodge_ram_1500_1999\", \"dodge_ram_1500_1998\", \"dodge_ram_1500_1997\", \"dodge_ram_1500_1996\", \"dodge_ram_1500_1995\"],\n",
    "    \"gmc_sierra_2012\": [\"gmc_sierra_1500_2007\", \"gmc_sierra_1500_2008\", \"gmc_sierra_1500_2009\", \"gmc_sierra_1500_2010\", \"gmc_sierra_1500_2011\", \"gmc_sierra_1500_2012\", \"gmc_sierra_1500_2013\", \"gmc_sierra_2500_2007\", \"gmc_sierra_2500_2008\", \"gmc_sierra_2500_2009\", \"gmc_sierra_2500_2010\", \"gmc_sierra_2500_2011\", \"gmc_sierra_2500_2012\", \"gmc_sierra_2500_2013\"],\n",
    "    \"chevrolet_impala_2008\": [\"chevrolet_impala_2007\", \"chevrolet_impala_2008\", \"chevrolet_impala_2009\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sETTING THE PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_path = \"D:/Final ADM/Extracted dataset\"\n",
    "stolen_cars_path = \"D:/Final ADM/Most_stolen_cars\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVE THE ALREADY EXISITING FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(stolen_cars_path):\n",
    "    shutil.rmtree(stolen_cars_path)\n",
    "else:\n",
    "    os.makedirs(stolen_cars_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory, car_list in cars.items():\n",
    "    print(\"Creating\", directory)\n",
    "    car_directory_name = os.path.join(stolen_cars_path, directory)\n",
    "    os.makedirs(car_directory_name)\n",
    "    for car in car_list:\n",
    "        path = os.path.join(full_dataset_path, car, \"\")\n",
    "        files = glob.glob(path + '*.jpg')\n",
    "        for file in files:\n",
    "            shutil.copy(file, car_directory_name)\n",
    "\n",
    "cars_utils.display_images(stolen_cars_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing .png and other corrupt files\n",
    "import os\n",
    "\n",
    "dir_name = \"D:/Final ADM/Most_stolen_cars\"\n",
    "test = os.listdir(dir_name)\n",
    "\n",
    "for item in test:\n",
    "    if item.endswith(\".png\"):\n",
    "        os.remove(os.path.join(dir_name, item))\n",
    "        \n",
    "for item in test:\n",
    "    if item.endswith(\".jpeg\"):\n",
    "        os.remove(os.path.join(dir_name, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking various files used in the data\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool()\n",
    "    image_list = glob.glob(stolen_cars_path + \"/*/*\")\n",
    "    pool.map(cars_utils.check_image, image_list)\n",
    "    pool.close()\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Pygal and galplot to visualize the data\n",
    "import pygal \n",
    "from IPython.display import display, HTML\n",
    "#Create function to display interactive plotting\n",
    "base_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "  <script type=\"text/javascript\" src=\"http://kozea.github.com/pygal.js/javascripts/svg.jquery.js\"></script>\n",
    "  <script type=\"text/javascript\" src=\"https://kozea.github.io/pygal.js/2.0.x/pygal-tooltips.min.js\"\"></script>\n",
    "  </head>\n",
    "  <body>\n",
    "    <figure>\n",
    "      {rendered_chart}\n",
    "    </figure>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "def galplot(chart):\n",
    "    rendered_chart = chart.render(is_unicode=True)\n",
    "    plot_html = base_html.format(rendered_chart=rendered_chart)\n",
    "    display(HTML(plot_html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_chart = pygal.Bar(height=300)\n",
    "line_chart.title = 'Stolen Car Class Distribution'\n",
    "for o in os.listdir(stolen_cars_path):\n",
    "    line_chart.add(o, len(os.listdir(os.path.join(stolen_cars_path, o))))\n",
    "    line_chart.render_to_file('D:/Final ADM/class_distribution.svg')\n",
    "galplot(line_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(stolen_cars_path):\n",
    "    level = root.replace(os.getcwd(), '').count(os.sep)\n",
    "    print('{0}{1}/'.format('    ' * level, os.path.basename(root)))\n",
    "    for f in files[:2]:\n",
    "        print('{0}{1}'.format('    ' * (level + 1), f))\n",
    "    if level is not 0:\n",
    "        print('{0}{1}'.format('    ' * (level + 1), \"...\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test Set Variables\n",
    "train_val_test_ratio = (.8,.05,.15) # 80/5/15 Data Split\n",
    "test_folder = 'D:/Final ADM/proc_data/test/'\n",
    "train_folder = 'D:/Final ADM/proc_data/train/'\n",
    "val_folder = 'D:/Final ADM/proc_data/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = os.listdir('D:/Final ADM/Most_stolen_cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Existing Folders\n",
    "for folder in [test_folder, train_folder, val_folder]:\n",
    "    if os.path.exists(folder) and os.path.isdir(folder):\n",
    "        shutil.rmtree(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in file_names:\n",
    "    os.makedirs(test_folder + category)\n",
    "    os.makedirs(train_folder + category)\n",
    "    os.makedirs(val_folder + category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, category in enumerate(file_names):\n",
    "    file_list = os.listdir(stolen_cars_path + '/' + category)\n",
    "    \n",
    "    train_ratio = math.floor(len(file_list) * train_val_test_ratio[0])\n",
    "    val_ratio = math.floor(len(file_list) * train_val_test_ratio[1])\n",
    "    train_list = file_list[:train_ratio]\n",
    "    val_list = file_list[train_ratio:train_ratio + val_ratio]\n",
    "    test_list = file_list[train_ratio + val_ratio:]\n",
    "    for i, file in enumerate(train_list):\n",
    "        shutil.copy(stolen_cars_path + '/' + category + '/' + file, train_folder + '/' + category + '/' + file)\n",
    "        sys.stdout.write('Moving %s train images to category folder %s' % (len(train_list), category))  \n",
    "        sys.stdout.write('\\n')\n",
    "    for i,file in enumerate(val_list):\n",
    "        shutil.copy(stolen_cars_path + '/' + category + '/' + file, val_folder + '/' + category + '/' + file)\n",
    "        sys.stdout.write('Moving %s validation images to category folder %s' % (len(val_list), category))                   \n",
    "        sys.stdout.write('\\n')\n",
    "    for i, file in enumerate(test_list):\n",
    "        shutil.copy(stolen_cars_path + '/' + category + '/' + file, test_folder + '/' + category + '/' + file)\n",
    "        sys.stdout.write('Moving %s test images to category folder %s' % (len(test_list), category))\n",
    "        sys.stdout.write('\\n')\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "#Select a random image and follow the next step\n",
    "datagen = ImageDataGenerator(rotation_range=45, \n",
    "                             width_shift_range=0.2, \n",
    "                             height_shift_range=0.2, \n",
    "                             zoom_range=0.3, \n",
    "                             vertical_flip=True,\n",
    "                             horizontal_flip=True, \n",
    "                             fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(\"D:/Final ADM/proc_data/test/*/*\") \n",
    "img_path = random.choice(file_list)\n",
    "img = load_img(img_path)\n",
    "car_class = img_path.split(\"/\")[1]\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\") \n",
    "plt.title(\"Original \" + car_class, fontsize=16)\n",
    "\n",
    "img = img_to_array(img)\n",
    "img = img.reshape((1,) + img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_augmentations = 4   #\"\"\"Change this to 3\"\"\"\n",
    "plt.figure(figsize=(15, 6))    \n",
    "i = 0\n",
    "for batch in datagen.flow(img, \n",
    "                          batch_size=1, \n",
    "                          seed=21):\n",
    "    \n",
    "    plt.subplot(2, int(np.ceil(n_augmentations * 1. / 2)), i + 1)\n",
    "    plt.imshow(array_to_img(batch[0]))\n",
    "    plt.axis(\"off\")\n",
    "    plt.suptitle(\"Augmented \" + car_class, fontsize=16)    \n",
    "    \n",
    "    i += 1\n",
    "    if i >= n_augmentations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(data_dir):\n",
    "    list_of_images = os.listdir(data_dir)\n",
    "    datagen = ImageDataGenerator(rotation_range=45, \n",
    "        horizontal_flip=True, \n",
    "        fill_mode=\"nearest\")\n",
    "    for img_name in list_of_images:\n",
    "        tmp_img_name = os.path.join(data_dir, img_name)\n",
    "        img = load_img(tmp_img_name)\n",
    "        img = img_to_array(img)\n",
    "        img = img.reshape((1,) + img.shape)\n",
    "\n",
    "        batch = datagen.flow(img, \n",
    "            batch_size=1, \n",
    "            seed=21,\n",
    "            save_to_dir=data_dir, \n",
    "            save_prefix=img_name.split(\".jpg\")[0] + \"augmented\", \n",
    "            save_format=\"jpg\")\n",
    "\n",
    "        batch.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_augment = [\n",
    "    \"toyota_camry_2014\",\n",
    "    \"nissan_altima_2014\",\n",
    "    \"toyota_corolla_2013\",\n",
    "    \"gmc_sierra_2012\",\n",
    "    ]\n",
    "\n",
    "for class_names in classes_to_augment:\n",
    "    print(\"Currently Augmenting:\", class_names)\n",
    "    data_dir = os.path.join(train_folder, class_names)\n",
    "    data_augment(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(file, size=299):\n",
    "    print(file)\n",
    "    img = Image.open(file)\n",
    "    img = img.resize((size,size))\n",
    "    img.save(file)\n",
    "\n",
    "#Resize Images\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool()\n",
    "    image_list = glob.glob(train_folder + \"/*/*\")\n",
    "    [resize_image(x) for x in image_list]\n",
    "    func = partial(resize_image, size=299)\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_utils.display_images(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_chart = pygal.Bar(height=300)\n",
    "line_chart.title = 'Most Stolen Car Training Class Distribution'\n",
    "for o in os.listdir(train_folder):\n",
    "    line_chart.add(o, len(os.listdir(os.path.join(train_folder, o))))\n",
    "galplot(line_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "WIDTH=224\n",
    "HEIGHT=224\n",
    "BATCH_SIZE=16\n",
    "test_dir = 'D:/Final ADM/proc_data/test/'\n",
    "train_dir = 'D:/Final ADM/proc_data/train/'\n",
    "val_dir = 'D:/Final ADM/proc_data/val/'\n",
    "\n",
    "#Train DataSet Generator with Augmentation\n",
    "print(\"\\nTraining Data Set\")\n",
    "train_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_flow = train_generator.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n",
    "\n",
    "#Validation DataSet Generator with Augmentation\n",
    "print(\"\\nValidation Data Set\")\n",
    "val_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_flow = val_generator.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    batch_size = BATCH_SIZE\n",
    ")\n",
    "\n",
    "#Test DataSet Generator with Augmentation\n",
    "print(\"\\nTest Data Set\")\n",
    "test_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_flow = test_generator.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(HEIGHT, WIDTH),\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VGG16 with transfer learning\n",
    "base_model = applications.VGG16(weights='imagenet', \n",
    "                                include_top=False, \n",
    "                                input_shape=(WIDTH, HEIGHT,3))\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# and a dense layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(len(train_flow.class_indices), activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional VGG16 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001), metrics=['accuracy', 'top_k_categorical_accuracy'], loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "top_layers_file_path=\"top_layers.vgg16.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(top_layers_file_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "tb = TensorBoard(log_dir='D:/Final ADM/LOGS/logs', batch_size=val_flow.batch_size, write_graph=True, update_freq='batch')\n",
    "early = EarlyStopping(monitor=\"loss\", mode=\"min\", patience=5)\n",
    "csv_logger = CSVLogger('D:/Final ADM/LOGS/logs/vgg16-log.csv', append=True)\n",
    "\n",
    "history = model.fit_generator(train_flow, \n",
    "                              epochs=170, \n",
    "                              verbose=1,\n",
    "                              validation_data=val_flow,\n",
    "                              validation_steps=math.ceil(val_flow.samples/val_flow.batch_size),\n",
    "                              steps_per_epoch=math.ceil(train_flow.samples/train_flow.batch_size),\n",
    "                              callbacks=[checkpoint, early, tb, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(top_layers_file_path)\n",
    "loss, acc, top_5 = model.evaluate_generator(\n",
    "    test_flow,\n",
    "    verbose = True,\n",
    "    steps=math.ceil(test_flow.samples/test_flow.batch_size))\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Acc: \", acc)\n",
    "print(\"Top 5: \", top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [k for k,v in train_flow.class_indices.items()]\n",
    "with open('vgg16-labels.txt', 'w+') as file:\n",
    "    file.write(\"\\n\".join(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "file_list = glob.glob(\"D:/Final ADM/proc_data/test/*/*\")\n",
    "img_path = random.choice(file_list)\n",
    "img_cat = os.path.split(os.path.dirname(img_path))[1]\n",
    "print(\"Image Category: \", img_cat)\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "print(\"Raw Predictions: \", preds)\n",
    "\n",
    "top_x = 3\n",
    "top_args = preds[0].argsort()[-top_x:][::-1]\n",
    "preds_label = [label[p] for p in top_args]\n",
    "print(\"\\nTop \" + str(top_x) + \" confidence: \" + \" \".join(map(str, sorted(preds[0])[-top_x:][::-1])))\n",
    "print(\"Top \" + str(top_x) + \" labels: \" + \" \".join(map(str, preds_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.framework import graph_io\n",
    "\n",
    "input_model_path = top_layers_file_path\n",
    "output_model_name = \"top_layers.vgg16.pb\"\n",
    "output_model_dir = \"tf_model\"\n",
    "\n",
    "K.set_learning_phase(0)\n",
    "sess = K.get_session()\n",
    "\n",
    "test_model = models.load_model(input_model_path)\n",
    "orig_output_node_names = [node.op.name for node in test_model.outputs]\n",
    "\n",
    "constant_graph = graph_util.convert_variables_to_constants(\n",
    "    sess,\n",
    "    sess.graph.as_graph_def(),\n",
    "    orig_output_node_names)\n",
    "graph_io.write_graph(\n",
    "    constant_graph,\n",
    "    output_model_dir,\n",
    "    output_model_name,\n",
    "    as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(top_layers_file_path)\n",
    "\n",
    "# unfreeze all layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.0001), metrics=['accuracy', 'top_k_categorical_accuracy'], loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Training top layers\n",
    "all_nodes_file_path=\"all_layers.vgg16.hdf5\"\n",
    "checkpoint = ModelCheckpoint(all_nodes_file_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "train_flow.reset()\n",
    "val_flow.reset()\n",
    "history = model.fit_generator(train_flow, \n",
    "                              epochs=170, \n",
    "                              verbose=1,\n",
    "                              validation_data = val_flow,\n",
    "                              validation_steps= math.ceil(val_flow.samples/val_flow.batch_size),\n",
    "                              steps_per_epoch = math.ceil(train_flow.samples/train_flow.batch_size),\n",
    "                              callbacks = [checkpoint, early, tb, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model #importing the model\n",
    "model = load_model('top_layers.vgg16.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    from keras.applications.vgg16 import preprocess_input\n",
    "    #Test DataSet Generator with Augmentation\n",
    "    test_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    test_flow = test_generator.flow_from_directory(\n",
    "        'D:/Final ADM/proc_data/test',\n",
    "        shuffle=False,\n",
    "        target_size=(224, 224),\n",
    "        batch_size = 16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import math\n",
    "    import numpy as np\n",
    "    predictions = model.predict_generator(\n",
    "        test_flow,\n",
    "        verbose=1,\n",
    "        steps=math.ceil(test_flow.samples/test_flow.batch_size))\n",
    "    predicted_classes = np.argmax(predictions, axis=1) \n",
    "    \n",
    "    true_classes = test_flow.classes\n",
    "    class_labels = list(test_flow.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(k, \":\", v) for k,v in enumerate(class_labels)]\n",
    "true_map_classes = [class_labels[x] for x in true_classes]\n",
    "predicted_map_classes = [class_labels[x] for x in predicted_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(\n",
    "        true_map_classes, \n",
    "        predicted_map_classes,\n",
    "        labels=class_labels,\n",
    "        x_tick_rotation=90,\n",
    "        figsize=(40, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(\n",
    "    true_classes,\n",
    "    predicted_classes,\n",
    "    target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_precision_recall(\n",
    "    true_classes,\n",
    "    predictions,\n",
    "    figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_roc(\n",
    "    true_classes,\n",
    "    predictions,\n",
    "    figsize = (12,12))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
